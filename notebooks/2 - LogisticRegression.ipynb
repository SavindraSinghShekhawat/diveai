{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from diveai.metrics import accuracy_score\n",
    "from diveai.models import LogisticRegression\n",
    "from diveai.visualization import PlotBuilder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing Logistic Regression Model with Dive Mode Enabled\n",
      "\n",
      "Step 1: Understanding the Logistic Function\n",
      "The logistic function (sigmoid) is used to map linear outputs to probabilities:\n",
      "σ(z) = 1 / (1 + e^(-z))\n",
      "This function maps any real number to the range (0, 1).\n",
      "\n",
      "Step 2: Cost Function for Logistic Regression\n",
      "The cost function used is Binary Cross-Entropy:\n",
      "J(w, b) = -(1/m) * Σ(y * log(y_pred) + (1 - y) * log(1 - y_pred))\n",
      "This measures the dissimilarity between true labels and predicted probabilities.\n",
      "\n",
      "Step 3: Deriving Gradients for Optimization\n",
      "The gradients for logistic regression are:\n",
      "∂J/∂w = (1/m) * X.T * (σ(X*w + b) - y)\n",
      "∂J/∂b = (1/m) * Σ(σ(X*w + b) - y)\n",
      "These gradients are used to update weights and bias in gradient descent.\n",
      "\n",
      "\n",
      "Step 4: Training Process Begins\n",
      "Number of Training Examples: 5, Features: 1\n",
      "Learning Rate: 0.1, Iterations: 2000\n",
      "Starting Gradient Descent...\n",
      "\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index 1 is out of bounds for axis 1 with size 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 9\u001b[0m\n\u001b[1;32m      6\u001b[0m model \u001b[38;5;241m=\u001b[39m LogisticRegression(learning_rate\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.1\u001b[39m, iterations\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2000\u001b[39m, dive\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[0;32m----> 9\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# Make predictions\u001b[39;00m\n\u001b[1;32m     12\u001b[0m predictions \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mpredict(X)\n",
      "File \u001b[0;32m/Volumes/Personal/DiveAI/diveai/models/logistic_regression.py:115\u001b[0m, in \u001b[0;36mLogisticRegression.fit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    112\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLearning Rate: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlearning_rate\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Iterations: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39miterations\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    113\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStarting Gradient Descent...\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 115\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweights, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias \u001b[38;5;241m=\u001b[39m \u001b[43mgradient_descent\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    116\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweights\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearning_rate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43miterations\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdive\u001b[49m\n\u001b[1;32m    117\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    119\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdive:\n\u001b[1;32m    120\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mTraining Complete\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/Volumes/Personal/DiveAI/diveai/models/logistic_regression.py:29\u001b[0m, in \u001b[0;36mgradient_descent\u001b[0;34m(X, y, initial_weights, initial_bias, learning_rate, iterations, dive)\u001b[0m\n\u001b[1;32m     27\u001b[0m pb\u001b[38;5;241m.\u001b[39madd_plot([], [], row\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, col\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, plot_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mline\u001b[39m\u001b[38;5;124m\"\u001b[39m, color\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124morange\u001b[39m\u001b[38;5;124m\"\u001b[39m, label\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWeights\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     28\u001b[0m pb\u001b[38;5;241m.\u001b[39madd_plot([], [], row\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, col\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, plot_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mline\u001b[39m\u001b[38;5;124m\"\u001b[39m, color\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgreen\u001b[39m\u001b[38;5;124m\"\u001b[39m, label\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBias\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 29\u001b[0m pb\u001b[38;5;241m.\u001b[39madd_plot(X[:, \u001b[38;5;241m0\u001b[39m], \u001b[43mX\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m, row\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, col\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m, plot_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mscatter\u001b[39m\u001b[38;5;124m\"\u001b[39m, color\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mblack\u001b[39m\u001b[38;5;124m\"\u001b[39m, label\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mData\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     30\u001b[0m \u001b[38;5;66;03m# pb.add_plot(X[:, 0], y.flatten(), row=0, col=2, plot_type=\"scatter\", color=\"black\", label=\"Data\")\u001b[39;00m\n\u001b[1;32m     31\u001b[0m pb\u001b[38;5;241m.\u001b[39madd_plot([], [], row\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, col\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m, plot_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mline\u001b[39m\u001b[38;5;124m\"\u001b[39m, color\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mred\u001b[39m\u001b[38;5;124m\"\u001b[39m, label\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDecision Boundary\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mIndexError\u001b[0m: index 1 is out of bounds for axis 1 with size 1"
     ]
    }
   ],
   "source": [
    "# Sample dataset (Years of Experience vs Promotion)\n",
    "X = np.array([[1], [2], [3], [4], [5]])  # Feature matrix (Years of Experience)\n",
    "y = np.array([0, 0, 0, 1, 1])  # Target vector (0: Not Promoted, 1: Promoted)\n",
    "\n",
    "# Initialize the Logistic Regression model\n",
    "model = LogisticRegression(learning_rate=0.1, iterations=2000, dive=True)\n",
    "\n",
    "# Train the model\n",
    "model.fit(X, y)\n",
    "\n",
    "# Make predictions\n",
    "predictions = model.predict(X)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = accuracy_score(y, predictions)\n",
    "\n",
    "# Print results\n",
    "print(\"Predictions:\", predictions.flatten())\n",
    "print(\"Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing Logistic Regression Model with Dive Mode Enabled\n",
      "\n",
      "Step 1: Understanding the Logistic Function\n",
      "The logistic function (sigmoid) is used to map linear outputs to probabilities:\n",
      "σ(z) = 1 / (1 + e^(-z))\n",
      "This function maps any real number to the range (0, 1).\n",
      "\n",
      "Step 2: Cost Function for Logistic Regression\n",
      "The cost function used is Binary Cross-Entropy:\n",
      "J(w, b) = -(1/m) * Σ(y * log(y_pred) + (1 - y) * log(1 - y_pred))\n",
      "This measures the dissimilarity between true labels and predicted probabilities.\n",
      "\n",
      "Step 3: Deriving Gradients for Optimization\n",
      "The gradients for logistic regression are:\n",
      "∂J/∂w = (1/m) * X.T * (σ(X*w + b) - y)\n",
      "∂J/∂b = (1/m) * Σ(σ(X*w + b) - y)\n",
      "These gradients are used to update weights and bias in gradient descent.\n",
      "\n",
      "\n",
      "Step 4: Training Process Begins\n",
      "Number of Training Examples: 8, Features: 2\n",
      "Learning Rate: 0.1, Iterations: 1000\n",
      "Starting Gradient Descent...\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "847460eae75f4686b61e2fb8cb70f61b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FigureWidget({\n",
       "    'data': [{'line': {'color': 'blue'},\n",
       "              'mode': 'lines',\n",
       "              'name': 'Cost',\n",
       "              'type': 'scatter',\n",
       "              'uid': '1f31c328-0e07-4dbd-a056-4f7505c906e1',\n",
       "              'x': [],\n",
       "              'xaxis': 'x',\n",
       "              'y': [],\n",
       "              'yaxis': 'y'},\n",
       "             {'line': {'color': 'orange'},\n",
       "              'mode': 'lines',\n",
       "              'name': 'Weights',\n",
       "              'type': 'scatter',\n",
       "              'uid': '9c8fadb0-8038-447b-9bd9-902dace1dff0',\n",
       "              'x': [],\n",
       "              'xaxis': 'x2',\n",
       "              'y': [],\n",
       "              'yaxis': 'y2'},\n",
       "             {'line': {'color': 'green'},\n",
       "              'mode': 'lines',\n",
       "              'name': 'Bias',\n",
       "              'type': 'scatter',\n",
       "              'uid': '706511c4-3574-47a6-bca2-f0874f3d7a6f',\n",
       "              'x': [],\n",
       "              'xaxis': 'x2',\n",
       "              'y': [],\n",
       "              'yaxis': 'y2'},\n",
       "             {'marker': {'color': 'black'},\n",
       "              'mode': 'markers',\n",
       "              'name': 'Data',\n",
       "              'type': 'scatter',\n",
       "              'uid': '40ea3488-240b-404d-81e5-9859d5561a34',\n",
       "              'x': array([2., 3., 4., 5., 2., 3., 4., 5.]),\n",
       "              'xaxis': 'x3',\n",
       "              'y': array([3. , 2.5, 3.5, 3.8, 2. , 3.2, 2.8, 3.9]),\n",
       "              'yaxis': 'y3'},\n",
       "             {'line': {'color': 'red'},\n",
       "              'mode': 'lines',\n",
       "              'name': 'Decision Boundary',\n",
       "              'type': 'scatter',\n",
       "              'uid': '018b5d62-3678-469b-bf28-72ebdf63de71',\n",
       "              'x': [],\n",
       "              'xaxis': 'x3',\n",
       "              'y': [],\n",
       "              'yaxis': 'y3'}],\n",
       "    'layout': {'annotations': [{'font': {'size': 16},\n",
       "                                'showarrow': False,\n",
       "                                'text': 'Cost vs Iterations',\n",
       "                                'x': 0.14444444444444446,\n",
       "                                'xanchor': 'center',\n",
       "                                'xref': 'paper',\n",
       "                                'y': 1.0,\n",
       "                                'yanchor': 'bottom',\n",
       "                                'yref': 'paper'},\n",
       "                               {'font': {'size': 16},\n",
       "                                'showarrow': False,\n",
       "                                'text': 'Weights and Bias vs Iterations',\n",
       "                                'x': 0.5,\n",
       "                                'xanchor': 'center',\n",
       "                                'xref': 'paper',\n",
       "                                'y': 1.0,\n",
       "                                'yanchor': 'bottom',\n",
       "                                'yref': 'paper'},\n",
       "                               {'font': {'size': 16},\n",
       "                                'showarrow': False,\n",
       "                                'text': 'Data & Decision Boundary',\n",
       "                                'x': 0.8555555555555556,\n",
       "                                'xanchor': 'center',\n",
       "                                'xref': 'paper',\n",
       "                                'y': 1.0,\n",
       "                                'yanchor': 'bottom',\n",
       "                                'yref': 'paper'}],\n",
       "               'template': '...',\n",
       "               'title': {'text': 'Logistic Regression Gradient Descent'},\n",
       "               'xaxis': {'anchor': 'y', 'domain': [0.0, 0.2888888888888889], 'title': {'text': 'Iterations'}},\n",
       "               'xaxis2': {'anchor': 'y2',\n",
       "                          'domain': [0.35555555555555557, 0.6444444444444445],\n",
       "                          'title': {'text': 'Iterations'}},\n",
       "               'xaxis3': {'anchor': 'y3', 'domain': [0.7111111111111111, 1.0], 'title': {'text': 'X'}},\n",
       "               'yaxis': {'anchor': 'x', 'domain': [0.0, 1.0], 'title': {'text': 'Cost (Binary Cross-Entropy)'}},\n",
       "               'yaxis2': {'anchor': 'x2', 'domain': [0.0, 1.0], 'title': {'text': 'Value'}},\n",
       "               'yaxis3': {'anchor': 'x3', 'domain': [0.0, 1.0], 'title': {'text': 'y'}}}\n",
       "})"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Complete\n",
      "Final Weights: [ 3.42421678 -2.56418931], Final Bias: -4.033394\n",
      "Predictions: [0 0 1 1 0 0 1 1]\n",
      "Accuracy: [1.]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Sample dataset with two features (e.g., Hours Studied and Previous GPA)\n",
    "X = np.array([\n",
    "    [2, 3.0],  # Hours, GPA\n",
    "    [3, 2.5],\n",
    "    [4, 3.5],\n",
    "    [5, 3.8],\n",
    "    [2, 2.0],\n",
    "    [3, 3.2],\n",
    "    [4, 2.8],\n",
    "    [5, 3.9]\n",
    "])\n",
    "\n",
    "# Target variable (Pass/Fail)\n",
    "y = np.array([0, 0, 1, 1, 0, 0, 1, 1])\n",
    "\n",
    "# Initialize and train the model\n",
    "model = LogisticRegression(learning_rate=0.1, iterations=1000, dive=True)\n",
    "model.fit(X, y)\n",
    "\n",
    "# Make predictions\n",
    "predictions = model.predict(X)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(y, predictions)\n",
    "print(\"Predictions:\", predictions.flatten())\n",
    "print(\"Accuracy:\", accuracy)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "diveai",
   "language": "python",
   "name": "diveai"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
